# Consistency Guardian environment configuration
# Copy to .env and customize as needed

# LLM provider: ollama | lm-studio | llama-cpp
LLM_PROVIDER=ollama

# Model name (for ollama or lm-studio)
LLM_MODEL=llama3

# Provider base URLs (if different from defaults)
# Ollama default: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# LM Studio default: http://localhost:1234
LMSTUDIO_BASE_URL=http://localhost:1234

# llama.cpp server default: http://localhost:8080
LLAMACPP_BASE_URL=http://localhost:8080

# Guardian behavior
# If set to false, the CLI will not exit with code 1 on critical issues.
CG_FAIL_ON_CRITICAL=true

# Request timeout for network checks (seconds)
CG_TIMEOUT_SECONDS=30
